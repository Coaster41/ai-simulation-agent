{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antas/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import autogluon \n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "# print(torch.cuda.current_device())\n",
    "# print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "device = torch.device('cpu') # bruh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107898, 101)\n",
      "(107898,)\n",
      "torch.Size([107898, 1, 10, 10]) torch.Size([107898, 1])\n",
      "torch.Size([72291, 1, 10, 10]) torch.Size([72291, 1])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('results/results_100_100000_new.csv', header=None)\n",
    "model_num = 1\n",
    "# display(df)\n",
    "results = df.to_numpy()\n",
    "print(results.shape)\n",
    "print(results[:,-1].shape)\n",
    "X = torch.tensor(results[:,:-1]).to(torch.float32).to(device)\n",
    "if model_num == 1:\n",
    "    X = torch.reshape(X,(X.shape[0],1,10,10))\n",
    "y = torch.tensor(np.expand_dims(results[:,-1], axis=1)).to(torch.float32).to(device)\n",
    "print(X.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "# Setting up Dataloader\n",
    "# training_dataset = TensorDataset(X_train, y_train)\n",
    "# train_loader = DataLoader(training_dataset, batch_size=24, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  (7): Linear(in_features=1568, out_features=30, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=30, out_features=15, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Linear(in_features=15, out_features=1, bias=True)\n",
      "  (12): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = X.shape[1]\n",
    "if model_num == 0:\n",
    "    model = nn.Sequential(nn.Linear(input_size, 64),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(64,32),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(32,32),\n",
    "                        nn.BatchNorm2d(),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(32, 1),\n",
    "                        nn.Sigmoid())\n",
    "else:\n",
    "    model = nn.Sequential(nn.Conv2d(1,64,2),\n",
    "                          nn.ReLU(),\n",
    "                          nn.BatchNorm2d(num_features=64),\n",
    "                          nn.Conv2d(64,32,3),\n",
    "                          nn.ReLU(),\n",
    "                          nn.BatchNorm2d(num_features=32),\n",
    "                          nn.Flatten(),\n",
    "                          nn.Linear(1568, 30),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(30, 15),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(15,1),\n",
    "                          nn.Sigmoid())\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "  y_pred = model(X_train)\n",
    "  loss = loss_function(y_pred, y_train)\n",
    "  losses.append(loss.item())\n",
    "\n",
    "  model.zero_grad()\n",
    "  loss.backward()\n",
    "\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.title(\"Learning rate %f\"%(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017766855657100677\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.110856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.709152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.656199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.360878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.039649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3295</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.073872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.182848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.964233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.446859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3299</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.078319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3300 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_test    y_pred\n",
       "0       0.17  0.110856\n",
       "1       0.93  0.709152\n",
       "2       0.88  0.656199\n",
       "3       0.60  0.360878\n",
       "4       0.01  0.039649\n",
       "...      ...       ...\n",
       "3295    0.01  0.073872\n",
       "3296    0.26  0.182848\n",
       "3297    0.91  0.964233\n",
       "3298    0.46  0.446859\n",
       "3299    0.04  0.078319\n",
       "\n",
       "[3300 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model(X_test)\n",
    "loss = loss_function(y_pred, y_test)\n",
    "print(loss.item())\n",
    "compiled_results = y_test.cpu().detach().numpy()\n",
    "compiled_results = np.append(compiled_results, y_pred.cpu().detach().numpy(), axis=1)\n",
    "results = pd.DataFrame(compiled_results, columns = ['y_test','y_pred'])\n",
    "pd.set_option('display.max_rows', 30)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230509_072347/\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (72291 samples, 58.12 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230509_072347/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.6\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #42~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Apr 18 17:40:00 UTC 2\n",
      "Train Data Rows:    72291\n",
      "Train Data Columns: 100\n",
      "Label Column: result\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1541.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.83 MB (3.8% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 100 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 100 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t100 features in original data used to generate 100 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 57.83 MB (3.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.89s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.03458245148082057, Train Rows: 69791, Val Rows: 2500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-0.325\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-0.325\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.2s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.175849\n",
      "[2000]\tvalid_set's rmse: 0.169656\n",
      "[3000]\tvalid_set's rmse: 0.168033\n",
      "[4000]\tvalid_set's rmse: 0.167565\n",
      "[5000]\tvalid_set's rmse: 0.167251\n",
      "[6000]\tvalid_set's rmse: 0.167295\n",
      "[7000]\tvalid_set's rmse: 0.167367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.1672\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.77s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.145766\n",
      "[2000]\tvalid_set's rmse: 0.13947\n",
      "[3000]\tvalid_set's rmse: 0.137906\n",
      "[4000]\tvalid_set's rmse: 0.137552\n",
      "[5000]\tvalid_set's rmse: 0.137413\n",
      "[6000]\tvalid_set's rmse: 0.137315\n",
      "[7000]\tvalid_set's rmse: 0.137268\n",
      "[8000]\tvalid_set's rmse: 0.137125\n",
      "[9000]\tvalid_set's rmse: 0.137107\n",
      "[10000]\tvalid_set's rmse: 0.137082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.137\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.73s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 118 due to low memory. Expected memory usage reduced from 37.86% -> 15.0% of available memory...\n",
      "\t-0.2792\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# need to convert to pandas dataframe and train\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# get label\u001b[39;00m\n\u001b[1;32m     17\u001b[0m label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 19\u001b[0m predictor \u001b[39m=\u001b[39m TabularPredictor(label\u001b[39m=\u001b[39;49mlabel, problem_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mregression\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(train_data)\n\u001b[1;32m     22\u001b[0m y_pred_auto \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39mpredict(test_data\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[label]))\n\u001b[1;32m     23\u001b[0m y_pred_auto\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/core/utils/decorators.py:30\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     gargs, gkwargs \u001b[39m=\u001b[39m g(\u001b[39m*\u001b[39mother_args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49mgargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:866\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    864\u001b[0m     aux_kwargs[\u001b[39m'\u001b[39m\u001b[39mfit_weighted_ensemble\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[0;32m--> 866\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mtrain_data, X_val\u001b[39m=\u001b[39;49mtuning_data, X_unlabeled\u001b[39m=\u001b[39;49munlabeled_data,\n\u001b[1;32m    867\u001b[0m                   holdout_frac\u001b[39m=\u001b[39;49mholdout_frac, num_bag_folds\u001b[39m=\u001b[39;49mnum_bag_folds, num_bag_sets\u001b[39m=\u001b[39;49mnum_bag_sets,\n\u001b[1;32m    868\u001b[0m                   num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[1;32m    869\u001b[0m                   hyperparameters\u001b[39m=\u001b[39;49mhyperparameters, core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs, aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[1;32m    870\u001b[0m                   time_limit\u001b[39m=\u001b[39;49mtime_limit, infer_limit\u001b[39m=\u001b[39;49minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    871\u001b[0m                   verbosity\u001b[39m=\u001b[39;49mverbosity, use_bag_holdout\u001b[39m=\u001b[39;49muse_bag_holdout)\n\u001b[1;32m    872\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_post_fit_vars()\n\u001b[1;32m    874\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_fit(\n\u001b[1;32m    875\u001b[0m     keep_only_best\u001b[39m=\u001b[39mkwargs[\u001b[39m'\u001b[39m\u001b[39mkeep_only_best\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    876\u001b[0m     refit_full\u001b[39m=\u001b[39mkwargs[\u001b[39m'\u001b[39m\u001b[39mrefit_full\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    880\u001b[0m     infer_limit\u001b[39m=\u001b[39minfer_limit,\n\u001b[1;32m    881\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/tabular/learner/abstract_learner.py:125\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mLearner is already fit.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_input(X\u001b[39m=\u001b[39mX, X_val\u001b[39m=\u001b[39mX_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 125\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X\u001b[39m=\u001b[39;49mX, X_val\u001b[39m=\u001b[39;49mX_val, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:118\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_metric \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39meval_metric\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[0;32m--> 118\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    119\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    120\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    121\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    122\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    123\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    124\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[1;32m    125\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit_trainer,\n\u001b[1;32m    126\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    127\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    128\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    129\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrainer_fit_kwargs\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_trainer(trainer\u001b[39m=\u001b[39mtrainer)\n\u001b[1;32m    132\u001b[0m time_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/tabular/trainer/auto_trainer.py:98\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_bag_holdout:\n\u001b[1;32m     86\u001b[0m         \u001b[39m# TODO: User could be intending to blend instead. Add support for blend stacking.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m         \u001b[39m#  This error message is necessary because when calculating out-of-fold predictions for user, we want to return them in the form given in train_data,\u001b[39;00m\n\u001b[1;32m     88\u001b[0m         \u001b[39m#  but if we merge train and val here, it becomes very confusing from a users perspective, especially because we reset index, making it impossible to match\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         \u001b[39m#  the original train_data to the out-of-fold predictions from `predictor.get_oof_pred_proba()`.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mX_val, y_val is not None, but bagged mode was specified. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     91\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39mIf calling from `TabularPredictor.fit()`, `tuning_data` should be None.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     92\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39mDefault bagged mode does not use tuning data / validation data. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39mspecify the following:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     96\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mpredictor.fit(..., tuning_data=tuning_data, use_bag_holdout=True)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_and_ensemble(X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m     99\u001b[0m                                y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    100\u001b[0m                                X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    101\u001b[0m                                y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    102\u001b[0m                                X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    103\u001b[0m                                hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[1;32m    104\u001b[0m                                num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[1;32m    105\u001b[0m                                time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[1;32m    106\u001b[0m                                core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[1;32m    107\u001b[0m                                aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[1;32m    108\u001b[0m                                infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    109\u001b[0m                                infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    110\u001b[0m                                groups\u001b[39m=\u001b[39;49mgroups)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2051\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[0;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[1;32m   2049\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_rows_val \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_val)\n\u001b[1;32m   2050\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_cols_train \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(X\u001b[39m.\u001b[39mcolumns))\n\u001b[0;32m-> 2051\u001b[0m model_names_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_multi_levels(X, y, hyperparameters\u001b[39m=\u001b[39;49mhyperparameters, X_val\u001b[39m=\u001b[39;49mX_val, y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m   2052\u001b[0m                                           X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled, level_start\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, level_end\u001b[39m=\u001b[39;49mnum_stack_levels\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, time_limit\u001b[39m=\u001b[39;49mtime_limit, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2053\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_model_names()) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2054\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAutoGluon did not successfully train any models\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:312\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    310\u001b[0m         core_kwargs_level[\u001b[39m'\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m core_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m'\u001b[39m, time_limit_core)\n\u001b[1;32m    311\u001b[0m         aux_kwargs_level[\u001b[39m'\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m'\u001b[39m, time_limit_aux)\n\u001b[0;32m--> 312\u001b[0m     base_model_names, aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level(\n\u001b[1;32m    313\u001b[0m         X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, X_val\u001b[39m=\u001b[39;49mX_val, y_val\u001b[39m=\u001b[39;49my_val, X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    314\u001b[0m         models\u001b[39m=\u001b[39;49mhyperparameters, level\u001b[39m=\u001b[39;49mlevel, base_model_names\u001b[39m=\u001b[39;49mbase_model_names,\n\u001b[1;32m    315\u001b[0m         core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs_level, aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs_level, name_suffix\u001b[39m=\u001b[39;49mname_suffix,\n\u001b[1;32m    316\u001b[0m         infer_limit\u001b[39m=\u001b[39;49minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    317\u001b[0m     )\n\u001b[1;32m    318\u001b[0m     model_names_fit \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m base_model_names \u001b[39m+\u001b[39m aux_models\n\u001b[1;32m    319\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_best \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(model_names_fit) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:429\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    427\u001b[0m     core_kwargs[\u001b[39m'\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m core_kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[1;32m    428\u001b[0m     aux_kwargs[\u001b[39m'\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[0;32m--> 429\u001b[0m core_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level_core(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, X_val\u001b[39m=\u001b[39;49mX_val, y_val\u001b[39m=\u001b[39;49my_val, X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled, models\u001b[39m=\u001b[39;49mmodels,\n\u001b[1;32m    430\u001b[0m                                         level\u001b[39m=\u001b[39;49mlevel, infer_limit\u001b[39m=\u001b[39;49minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size, base_model_names\u001b[39m=\u001b[39;49mbase_model_names, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcore_kwargs)\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m X_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_new_level_aux(X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my, base_model_names\u001b[39m=\u001b[39mcore_models, level\u001b[39m=\u001b[39mlevel\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    434\u001b[0m                                           infer_limit\u001b[39m=\u001b[39minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39minfer_limit_batch_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39maux_kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:520\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m fit_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[1;32m    519\u001b[0m \u001b[39m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi(X\u001b[39m=\u001b[39;49mX_init, y\u001b[39m=\u001b[39;49my, X_val\u001b[39m=\u001b[39;49mX_val, y_val\u001b[39m=\u001b[39;49my_val, X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    521\u001b[0m                          models\u001b[39m=\u001b[39;49mmodels, level\u001b[39m=\u001b[39;49mlevel, stack_name\u001b[39m=\u001b[39;49mstack_name, compute_score\u001b[39m=\u001b[39;49mcompute_score, fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2021\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[0;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2019\u001b[0m \u001b[39mif\u001b[39;00m n_repeat_start \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2020\u001b[0m     time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m-> 2021\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_initial(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, models\u001b[39m=\u001b[39;49mmodels, k_fold\u001b[39m=\u001b[39;49mk_fold, n_repeats\u001b[39m=\u001b[39;49mn_repeats_initial, hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs,\n\u001b[1;32m   2022\u001b[0m                                                     feature_prune_kwargs\u001b[39m=\u001b[39;49mfeature_prune_kwargs, time_limit\u001b[39m=\u001b[39;49mtime_limit, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2023\u001b[0m     n_repeat_start \u001b[39m=\u001b[39m n_repeats_initial\n\u001b[1;32m   2024\u001b[0m     \u001b[39mif\u001b[39;00m time_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1913\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[0;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m bagged:\n\u001b[1;32m   1912\u001b[0m     time_ratio \u001b[39m=\u001b[39m hpo_time_ratio \u001b[39mif\u001b[39;00m hpo_enabled \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1913\u001b[0m     models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_fold(models\u001b[39m=\u001b[39;49mmodels, hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs,\n\u001b[1;32m   1914\u001b[0m                                     time_limit\u001b[39m=\u001b[39;49mtime_limit, time_split\u001b[39m=\u001b[39;49mtime_split, time_ratio\u001b[39m=\u001b[39;49mtime_ratio, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_args)\n\u001b[1;32m   1915\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1916\u001b[0m     time_ratio \u001b[39m=\u001b[39m hpo_time_ratio \u001b[39mif\u001b[39;00m hpo_enabled \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1992\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[0;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1990\u001b[0m         time_start_model \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   1991\u001b[0m         time_left \u001b[39m=\u001b[39m time_limit \u001b[39m-\u001b[39m (time_start_model \u001b[39m-\u001b[39m time_start)\n\u001b[0;32m-> 1992\u001b[0m model_name_trained_lst \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single_full(X, y, model, time_limit\u001b[39m=\u001b[39;49mtime_left, hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs_model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1994\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m   1995\u001b[0m     \u001b[39mdel\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1810\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[0;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m         bagged_model_fit_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_bagged_model_fit_kwargs(k_fold\u001b[39m=\u001b[39mk_fold, k_fold_start\u001b[39m=\u001b[39mk_fold_start, k_fold_end\u001b[39m=\u001b[39mk_fold_end, n_repeats\u001b[39m=\u001b[39mn_repeats, n_repeat_start\u001b[39m=\u001b[39mn_repeat_start)\n\u001b[1;32m   1809\u001b[0m         model_fit_kwargs\u001b[39m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[0;32m-> 1810\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_and_save(\n\u001b[1;32m   1811\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m   1812\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m   1813\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1814\u001b[0m         X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m   1815\u001b[0m         y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m   1816\u001b[0m         X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m   1817\u001b[0m         stack_name\u001b[39m=\u001b[39;49mstack_name,\n\u001b[1;32m   1818\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   1819\u001b[0m         compute_score\u001b[39m=\u001b[39;49mcompute_score,\n\u001b[1;32m   1820\u001b[0m         total_resources\u001b[39m=\u001b[39;49mtotal_resources,\n\u001b[1;32m   1821\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs\n\u001b[1;32m   1822\u001b[0m     )\n\u001b[1;32m   1823\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[1;32m   1824\u001b[0m \u001b[39mreturn\u001b[39;00m model_names_trained\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1502\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[0;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs)\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single(X, y, model, X_val, y_val, total_resources\u001b[39m=\u001b[39;49mtotal_resources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs)\n\u001b[1;32m   1504\u001b[0m fit_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   1505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_evaluation:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1447\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[0;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train_single\u001b[39m(\u001b[39mself\u001b[39m, X, y, model: AbstractModel, X_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, total_resources\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AbstractModel:\n\u001b[1;32m   1443\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1444\u001b[0m \u001b[39m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m \u001b[39m    Returns trained model object.\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1447\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, X_val\u001b[39m=\u001b[39;49mX_val, y_val\u001b[39m=\u001b[39;49my_val, total_resources\u001b[39m=\u001b[39;49mtotal_resources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs)\n\u001b[1;32m   1448\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py:703\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_fit_resources(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    702\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_memory_usage(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 703\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py:212\u001b[0m, in \u001b[0;36mCatBoostModel._fit\u001b[0;34m(self, X, y, X_val, y_val, time_limit, num_gpus, num_cpus, sample_weight, sample_weight_val, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m eval_set \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     fit_final_kwargs[\u001b[39m'\u001b[39m\u001b[39muse_best_model\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_final_kwargs)\n\u001b[1;32m    214\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams_trained[\u001b[39m'\u001b[39m\u001b[39miterations\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtree_count_\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/catboost/core.py:5730\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   5728\u001b[0m     CatBoostRegressor\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m-> 5730\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline,\n\u001b[1;32m   5731\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[1;32m   5732\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[1;32m   5733\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/catboost/core.py:2355\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2351\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2353\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[1;32m   2354\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2355\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[1;32m   2356\u001b[0m         train_pool,\n\u001b[1;32m   2357\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   2358\u001b[0m         params,\n\u001b[1;32m   2359\u001b[0m         allow_clear_pool,\n\u001b[1;32m   2360\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m   2361\u001b[0m     )\n\u001b[1;32m   2363\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/catboost/core.py:1759\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1758\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1759\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1760\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:4623\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4672\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from autogluon docs\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset\n",
    "X_train_flat = X_train.flatten(start_dim=1)\n",
    "\n",
    "train_data = pd.DataFrame(X_train_flat).astype('float')\n",
    "train_data['result'] = y_train\n",
    "\n",
    "X_test_flat = X_test.flatten(start_dim = 1)\n",
    "test_data = pd.DataFrame(X_test_flat).astype('float')\n",
    "test_data['result'] = y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# need to convert to pandas dataframe and train\n",
    "# get label\n",
    "label = 'result'\n",
    "\n",
    "predictor = TabularPredictor(label=label, problem_type='regression').fit(train_data)\n",
    "\n",
    "\n",
    "y_pred_auto = predictor.predict(test_data.drop(columns=[label]))\n",
    "y_pred_auto.head()\n",
    "\n",
    "predictor.evaluate(test_data, silent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
